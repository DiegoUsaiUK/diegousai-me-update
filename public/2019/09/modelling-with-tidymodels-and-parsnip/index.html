<!DOCTYPE html>
<html lang="en-UK" />
<head>
    <meta http-equiv="Content-Type" content="text/html" charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />

    <title>Modelling with Tidymodels and Parsnip - A Tidy Approach to a Classification Problem &middot; </title>

    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <link rel="shortcut icon" href="/favicon.ico" />
    <link rel="canonical" href="/2019/09/modelling-with-tidymodels-and-parsnip/" />

     <meta name="description" content="Recently I have completed the online course Business Analysis With R focused on applied data and business science with R, which introduced me to a couple of new" /> 

     
    
    <meta name="twitter:card" content="summary_large_image"/>
    <meta name="twitter:image" content="/img/karim-ghantous-vIZGAYbOl30-unsplash.jpg"/>
    
 
    <meta name="twitter:title" content="Modelling with Tidymodels and Parsnip - A Tidy Approach to a Classification Problem"/>
    <meta name="twitter:description" content="Recently I have completed the online course Business Analysis With R focused on applied data and business science with R, which introduced me to a couple of new"/>
    <meta name="twitter:url" content="/2019/09/modelling-with-tidymodels-and-parsnip/" />
    <meta name="twitter:site" content="@"/>

    <meta property="og:site_name" content="" />
    <meta property="og:title" content="Modelling with Tidymodels and Parsnip - A Tidy Approach to a Classification Problem &middot; Lifelong Learning" />
    <meta property="og:url" content="/2019/09/modelling-with-tidymodels-and-parsnip/" />
    

    <meta property="og:type" content="article" />
    <meta property="og:description" content="Recently I have completed the online course Business Analysis With R focused on applied data and business science with R, which introduced me to a couple of new" />

    <meta property="article:published_time" content="2019-09-22T00:00:00Z" />
    <meta property="article:tag" content="Machine Learning" /><meta property="article:tag" content="Tidyverse" /><meta property="article:tag" content="Classification" /><meta property="article:tag" content="Churn" /><meta property="article:tag" content="API" />

    <meta property="og:image" content="/img/karim-ghantous-vIZGAYbOl30-unsplash.jpg"/>


    <meta name="generator" content="Hugo 0.58.3" />

    <!-- Stylesheets -->
    <link rel="stylesheet" type="text/css" href="/built/screen.css" /> 
    <link rel="stylesheet" type="text/css" href="/css/casper-two.css" /> 
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" />
    

     

</head>


<body class="post-template">
  <div class="site-wrapper"> 

<header class="site-header outer">
  <div class="inner">
    <nav class="site-nav">
      <div class="site-nav-left">

        <ul class="nav" role="menu">
        
        
        
            <li class="" role="menuitem">
              <a href="/">Home</a>
            </li>
        
            <li class="" role="menuitem">
              <a href="/post">Articles</a>
            </li>
        
            <li class="" role="menuitem">
              <a href="/tags/">Tags</a>
            </li>
        
            <li class="" role="menuitem">
              <a href="/page/about/">About</a>
            </li>
        
      </ul></div>

      <div class="site-nav-right">
        <div class="social-links">
                    

                    

                    <a class="social-link" href="https://github.com/DiegoUsaiUK" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg></a>

                    <a class="social-link" href="https://www.linkedin.com/in/diegousaiuk" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 50 512 512"><path d="M150.65,100.682c0,27.992-22.508,50.683-50.273,50.683c-27.765,0-50.273-22.691-50.273-50.683 C50.104,72.691,72.612,50,100.377,50C128.143,50,150.65,72.691,150.65,100.682z M143.294,187.333H58.277V462h85.017V187.333z M279.195,187.333h-81.541V462h81.541c0,0,0-101.877,0-144.181c0-38.624,17.779-61.615,51.807-61.615 c31.268,0,46.289,22.071,46.289,61.615c0,39.545,0,144.181,0,144.181h84.605c0,0,0-100.344,0-173.915 s-41.689-109.131-99.934-109.131s-82.768,45.369-82.768,45.369V187.333z" /></svg></a>

                    <a class="social-link" href="https://medium.com/@diegousaiuk" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 195 195"><path d="M46.5340803,65.2157554 C46.6968378,63.6076572 46.0836,62.018231 44.8828198,60.93592 L32.6512605,46.2010582 L32.6512605,44 L70.6302521,44 L99.9859944,108.380952 L125.794585,44 L162,44 L162,46.2010582 L151.542017,56.2281011 C150.640424,56.9153477 150.193188,58.0448862 150.380019,59.1628454 L150.380019,132.837155 C150.193188,133.955114 150.640424,135.084652 151.542017,135.771899 L161.755369,145.798942 L161.755369,148 L110.38282,148 L110.38282,145.798942 L120.963119,135.527337 C122.002801,134.487948 122.002801,134.182246 122.002801,132.592593 L122.002801,73.0417402 L92.585901,147.755438 L88.6106443,147.755438 L54.3622782,73.0417402 L54.3622782,123.115814 C54.0767278,125.221069 54.7759199,127.3406 56.2581699,128.863022 L70.0186741,145.55438 L70.0186741,147.755438 L31,147.755438 L31,145.55438 L44.7605042,128.863022 C46.2319621,127.338076 46.8903838,125.204485 46.5340803,123.115814 L46.5340803,65.2157554 Z"/></svg></a>
        </div>  
            
      </div>

    </nav>  

  </div>
</header>

<main id="site-main" class="site-main outer" role="main">
  <div class="inner">
    
      <article class="post-full post"> 
    <header class="post-full-header">
        <section class="post-full-meta">
            <time class="post-full-meta-date" datetime="2019-09-22">22 September 2019</time>
                <span class="date-divider">/</span> <a href="/tags/machine-learning/">#Machine Learning</a>&nbsp;<a href="/tags/tidyverse/">#Tidyverse</a>&nbsp;
        </section>
        <h1 class="post-full-title">Modelling with Tidymodels and Parsnip - A Tidy Approach to a Classification Problem</h1>
    </header>
    
    <figure class="post-full-image" style="background-image: url(/img/karim-ghantous-vIZGAYbOl30-unsplash.jpg)">
    </figure>

    <section class="post-full-content">
        <div class="kg-card-markdown">
            



<p>Recently I have completed the online course <a href="https://university.business-science.io/p/ds4b-101-r-business-analysis-r"><strong>Business Analysis With R</strong></a> focused on <em>applied data and business science with R</em>, which introduced me to a couple of new modelling concepts and approaches. One that especially captured my attention is <code>parsnip</code> and its attempt to implement a unified modelling and analysis interface (similar to <strong>python’s</strong> <code>scikit-learn</code>) to seamlessly access several modelling platforms in R.</p>
<p><code>parsnip</code> is the brainchild of RStudio’s <a href="https://twitter.com/topepos"><strong>Max Khun</strong></a> (of <code>caret</code> fame) and <a href="https://twitter.com/dvaughan32"><strong>Davis Vaughan</strong></a> and forms part of <code>tidymodels</code>, a growing ensemble of tools to explore and iterate modelling tasks that shares a common philosophy (and a few libraries) with the <code>tidyverse</code>.</p>
<p>Although there are a number of packages at different stages in their development, I have decided to take <code>tidymodels</code> “for a spin”, so to speak, and create and execute a “tidy” modelling workflow to tackle a <strong>classification</strong> problem. My aim is to show how easy it is to fit a simple <strong>logistic regression</strong> in R’s <code>glm</code> and quickly switch to a cross-validated <strong>random forest</strong> using the <code>ranger</code> engine by changing only a few lines of code.</p>
<p>For this post in particular I’m focusing on four different libraries from the <code>tidymodels</code> suite: <code>rsample</code> for data sampling and cross-validation, <code>recipes</code> for data preprocessing, <code>parsnip</code> for model set up and estimation, and <code>yardstick</code> for model assessment.</p>
<p><strong>Note that</strong> the focus is on modelling workflow and libraries interaction. For that reason, I am keeping data exploration and feature engineering to a minimum.</p>
<div id="set-up" class="section level2">
<h2>Set up</h2>
<p>First, I load the packages I need for this analysis.</p>
<pre class="r"><code>library(tidymodels)
library(skimr)
library(tibble)</code></pre>
<p>For this project I am using the <a href="https://www.ibm.com/communities/analytics/watson-analytics-blog/predictive-insights-in-the-telco-customer-churn-data-set/"><strong>Telco Customer Churn</strong></a> from <a href="https://www.ibm.com/communities/analytics/watson-analytics/">IBM Watson Analytics</a>, one of IBM Analytics Communities. The data contains 7,043 rows, each representing a customer, and 21 columns for the potential predictors, providing information to forecast customer behaviour and help develop focused customer retention programmes.</p>
<p><code>Churn</code> is the <strong>Dependent Variable</strong> and shows the customers who left within the last month. The dataset also includes details on the <strong>Services</strong> that each customer has signed up for, along with <strong>Customer Account</strong> and <strong>Demographic</strong> information.</p>
<pre class="r"><code>telco &lt;- readr::read_csv(&quot;WA_Fn-UseC_-Telco-Customer-Churn.csv&quot;)</code></pre>
<pre class="r"><code>telco %&gt;% 
  skimr::skim()</code></pre>
<pre><code>## Skim summary statistics
##  n obs: 7043 
##  n variables: 21 
## 
## -- Variable type:character -------------------------------------------------------
##          variable missing complete    n min max empty n_unique
##             Churn       0     7043 7043   2   3     0        2
##          Contract       0     7043 7043   8  14     0        3
##        customerID       0     7043 7043  10  10     0     7043
##        Dependents       0     7043 7043   2   3     0        2
##  DeviceProtection       0     7043 7043   2  19     0        3
##            gender       0     7043 7043   4   6     0        2
##   InternetService       0     7043 7043   2  11     0        3
##     MultipleLines       0     7043 7043   2  16     0        3
##      OnlineBackup       0     7043 7043   2  19     0        3
##    OnlineSecurity       0     7043 7043   2  19     0        3
##  PaperlessBilling       0     7043 7043   2   3     0        2
##           Partner       0     7043 7043   2   3     0        2
##     PaymentMethod       0     7043 7043  12  25     0        4
##      PhoneService       0     7043 7043   2   3     0        2
##   StreamingMovies       0     7043 7043   2  19     0        3
##       StreamingTV       0     7043 7043   2  19     0        3
##       TechSupport       0     7043 7043   2  19     0        3
## 
## -- Variable type:numeric ---------------------------------------------------------
##        variable missing complete    n    mean      sd    p0    p25     p50
##  MonthlyCharges       0     7043 7043   64.76   30.09 18.25  35.5    70.35
##   SeniorCitizen       0     7043 7043    0.16    0.37  0      0       0   
##          tenure       0     7043 7043   32.37   24.56  0      9      29   
##    TotalCharges      11     7032 7043 2283.3  2266.77 18.8  401.45 1397.47
##      p75    p100
##    89.85  118.75
##     0       1   
##    55      72   
##  3794.74 8684.8</code></pre>
<p>There are a couple of things to notice here:</p>
<ul>
<li><p><strong>customerID</strong> is a unique identifier for each row. As such it has no descriptive or predictive power and it needs to be removed.</p></li>
<li><p>Given the relative small number of missing values in <strong>TotalCharges</strong> (only 11 of them) I am dropping them from the dataset.</p></li>
</ul>
<pre class="r"><code>telco &lt;- 
  telco %&gt;%
  select(-customerID) %&gt;%
  drop_na()</code></pre>
</div>
<div id="modelling-with-tidymodels" class="section level2">
<h2>Modelling with <code>tidymodels</code></h2>
<p>To show the basic steps in the <code>tidymodels</code> framework I am fitting and evaluating a simple <strong>logistic regression</strong> model.</p>
<div id="train-and-test-split" class="section level3">
<h3>Train and test split</h3>
<p><code>rsample</code> provides a streamlined way to create a randomised training and test split of the original data.</p>
<pre class="r"><code>set.seed(seed = 1972) 

train_test_split &lt;-
  rsample::initial_split(
    data = telco,     
    prop = 0.80   
  ) 

train_test_split
## &lt;5626/1406/7032&gt;</code></pre>
<p>Of the 7,043 total customers, 5,626 have been assigned to the training set and 1,406 to the test set. I save them as <code>train_tbl</code> and <code>test_tbl</code>.</p>
<pre class="r"><code>train_tbl &lt;- train_test_split %&gt;% training() 
test_tbl  &lt;- train_test_split %&gt;% testing() </code></pre>
</div>
<div id="a-simple-recipe" class="section level3">
<h3>A simple recipe</h3>
<p>The <code>recipes</code> package uses a <strong>cooking metaphor</strong> to handle all the data preprocessing, like missing values imputation, removing predictors, centring and scaling, one-hot-encoding, and more.</p>
<p>First, I create a <code>recipe</code> where I define the transformations I want to apply to my data. In this case I create a simple recipe to change all character variables to factors.</p>
<p>Then, I <em>“prep the recipe”</em> by mixing the ingredients with <code>prep</code>. Here I have included the prep bit in the recipe function for brevity.</p>
<pre class="r"><code>recipe_simple &lt;- function(dataset) {
  recipe(Churn ~ ., data = dataset) %&gt;%
    step_string2factor(all_nominal(), -all_outcomes()) %&gt;%
    prep(data = dataset)
}</code></pre>
<p><strong>Note that</strong> in order to avoid <em>data leakage</em> (e.g: transferring information from the train set into the test set), data should be “prepped” using the <code>train_tbl</code> only.</p>
<pre class="r"><code>recipe_prepped &lt;- recipe_simple(dataset = train_tbl)</code></pre>
<p>Finally, to continue with the cooking metaphor, I <em>“bake the recipe”</em> to apply all preprocessing to the data sets.</p>
<pre class="r"><code>train_baked &lt;- bake(recipe_prepped, new_data = train_tbl)
test_baked  &lt;- bake(recipe_prepped, new_data = test_tbl)</code></pre>
</div>
<div id="fit-the-model" class="section level3">
<h3>Fit the model</h3>
<p><code>parsnip</code> is a relatively recent addition to the <code>tidymodels</code> suite and is probably the one I like best. This package offers a unified API that allows access to several machine learning packages without the need to learn the syntax of each individual one.</p>
<p>With 3 simple steps you can:</p>
<ul>
<li><p>set the <strong>type of model</strong> you want to fit (here is a <code>logistic regression</code>) and its <strong>mode</strong> (<code>classification</code>)</p></li>
<li><p>decide which computational <strong>engine</strong> to use (<code>glm</code> in this case)</p></li>
<li><p>spell out the exact model specification to <strong>fit</strong> (I’m using all variables here) and what <strong>data</strong> to use (the baked train dataset)</p></li>
</ul>
<pre class="r"><code>logistic_glm &lt;-
  logistic_reg(mode = &quot;classification&quot;) %&gt;%
  set_engine(&quot;glm&quot;) %&gt;%
  fit(Churn ~ ., data = train_baked)</code></pre>
<p>If you want to use another engine you can simply switch the <code>set_engine</code> argument (for <em>logistic regression</em> you can choose from <code>glm</code>, <code>glmnet</code>, <code>stan</code>, <code>spark</code>, and <code>keras</code>) and <code>parsnip</code> will take care of changing everything else for you behind the scenes.</p>
</div>
<div id="performance-assessment" class="section level3">
<h3>Performance assessment</h3>
<p>The <code>yardstick</code> package provides an easy way to calculate several assessment measures. But before I can evaluate my model’s performance, I need to calculate some predictions by passing the <code>test_baked</code> data to the <code>predict</code> function.</p>
<pre class="r"><code>predictions_glm &lt;- logistic_glm %&gt;%
  predict(new_data = test_baked) %&gt;%
  bind_cols(test_baked %&gt;% select(Churn))

head(predictions_glm)
## # A tibble: 6 x 2
##   .pred_class Churn
##   &lt;fct&gt;       &lt;fct&gt;
## 1 Yes         No   
## 2 No          No   
## 3 No          No   
## 4 No          No   
## 5 No          No   
## 6 No          No</code></pre>
<p>There are several metrics that can be used to investigate the performance of a classification model but for simplicity I’m only focusing on a selection of them: <strong>accuracy</strong>, <strong>precision</strong>, <strong>recall</strong> and <strong>F1_Score</strong>.</p>
<p>All of these measures (and many more) can be derived by the <a href="https://en.wikipedia.org/wiki/Confusion_matrix"><strong>Confusion Matrix</strong></a>, a table used to describe the performance of a classification model on a set of test data for which the true values are known.</p>
<p>In and of itself, the confusion matrix is a relatively easy concept to get your head around as is shows the number of <em>false positives</em>, <em>false negatives</em>, <em>true positives</em>, and <em>true negatives</em>. However some of the measures that are derived from it may take some reasoning with to fully understand their meaning and use.</p>
<pre class="r"><code>predictions_glm %&gt;%
  conf_mat(Churn, .pred_class) %&gt;%
  pluck(1) %&gt;%
  as_tibble() %&gt;%
  ggplot(aes(Prediction, Truth, alpha = n)) +
  geom_tile(show.legend = FALSE) +
  geom_text(aes(label = n), colour = &quot;white&quot;, alpha = 1, size = 8)</code></pre>
<p><img src="/post/Tidymodels_Classification_Churn_files/figure-html/unnamed-chunk-10-1.png" width="80%" height="100%" style="display: block; margin: auto;" /></p>
<p>The model’s <strong>Accuracy</strong> is the fraction of predictions the model got right and can be easily calculated by passing the <code>predictions_glm</code> to the <code>metrics</code> function. However, accuracy is not a very reliable metric as it will provide misleading results if the data set is unbalanced.</p>
<p>With only basic data manipulation and feature engineering the simple logistic model has achieved 80% accuracy.</p>
<pre class="r"><code>predictions_glm %&gt;%
  metrics(Churn, .pred_class) %&gt;%
  select(-.estimator) %&gt;%
  filter(.metric == &quot;accuracy&quot;) %&gt;%
  kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="center">.metric</th>
<th align="center">.estimate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">accuracy</td>
<td align="center">0.8058321</td>
</tr>
</tbody>
</table>
<p><strong>Precision</strong> shows how sensitive models are to <em>False Positives</em> (i.e. predicting a customer is leaving when he-she is actually staying) whereas <strong>Recall</strong> looks at how sensitive models are to <em>False Negatives</em> (i.e. forecasting that a customer is staying whilst he-she is in fact leaving).</p>
<p>These are <strong>very relevant business metrics</strong> because organisations are particularly interested in accurately predicting which customers are truly at risk of leaving so that they can target them with retention strategies. At the same time they want to minimising efforts of retaining customers incorrectly classified as leaving who are instead staying.</p>
<pre class="r"><code>tibble(
  &quot;precision&quot; = 
     precision(predictions_glm, Churn, .pred_class) %&gt;%
     select(.estimate),
  &quot;recall&quot; = 
     recall(predictions_glm, Churn, .pred_class) %&gt;%
     select(.estimate)
) %&gt;%
  unnest() %&gt;%
  kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="center">precision</th>
<th align="center">recall</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">0.8466368</td>
<td align="center">0.9024857</td>
</tr>
</tbody>
</table>
<p>Another popular performance assessment metric is the <a href="https://en.wikipedia.org/wiki/F1_score"><strong>F1 Score</strong></a>, which is the harmonic average of the <a href="https://en.wikipedia.org/wiki/Precision_(information_retrieval)">precision</a> and <a href="https://en.wikipedia.org/wiki/Recall_(information_retrieval)">recall</a>. An F1 score reaches its best value at 1 with perfect <em>precision</em> and <em>recall</em>.</p>
<pre class="r"><code>predictions_glm %&gt;%
  f_meas(Churn, .pred_class) %&gt;%
  select(-.estimator) %&gt;%
  kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="center">.metric</th>
<th align="center">.estimate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">f_meas</td>
<td align="center">0.8736696</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="a-random-forest" class="section level2">
<h2>A Random Forest</h2>
<p>This is where the real beauty of <code>tidymodels</code> comes into play. Now I can use this tidy modelling framework to fit a <strong>Random Forest</strong> model with the <code>ranger</code> engine.</p>
<div id="cross-validation-set-up" class="section level3">
<h3>Cross-validation set up</h3>
<p>To further refine the model’s predictive power, I am implementing a <strong>10-fold cross validation</strong> using <code>vfold_cv</code> from <code>rsample</code>, which splits again the initial training data.</p>
<pre class="r"><code>cross_val_tbl &lt;- vfold_cv(train_tbl, v = 10)

cross_val_tbl
## #  10-fold cross-validation 
## # A tibble: 10 x 2
##    splits             id    
##    &lt;named list&gt;       &lt;chr&gt; 
##  1 &lt;split [5.1K/563]&gt; Fold01
##  2 &lt;split [5.1K/563]&gt; Fold02
##  3 &lt;split [5.1K/563]&gt; Fold03
##  4 &lt;split [5.1K/563]&gt; Fold04
##  5 &lt;split [5.1K/563]&gt; Fold05
##  6 &lt;split [5.1K/563]&gt; Fold06
##  7 &lt;split [5.1K/562]&gt; Fold07
##  8 &lt;split [5.1K/562]&gt; Fold08
##  9 &lt;split [5.1K/562]&gt; Fold09
## 10 &lt;split [5.1K/562]&gt; Fold10</code></pre>
<p>If we take a further look, we should recognise the 5,626 number, which is the total number of observations in the initial <code>train_tbl</code>. In each round, 563 observations will in turn be retained from estimation and used to validate the model for that fold.</p>
<pre class="r"><code>cross_val_tbl$splits %&gt;%
  pluck(1)
## &lt;5063/563/5626&gt;</code></pre>
<p>To avoid confusion and distinguish the <em>initial train/test splits</em> from those used for cross validation, the author of <code>rsample</code> <strong>Max Kuhn</strong> has coined two new terms: the <code>analysis</code> and the <code>assessment</code> sets. The former is the portion of the train data used to recursively estimate the model, where the latter is the portion used to validate each estimate.</p>
</div>
<div id="update-the-recipe" class="section level3">
<h3>Update the recipe</h3>
<p><strong>NOTE</strong> that a random forest needs <em>all numeric</em> variables to be <em>centred and scaled</em> and all <em>character/factor</em> variables to be <em>“dummified”</em>. This is easily done by updating the recipe with these transformations.</p>
<pre class="r"><code>recipe_rf &lt;- function(dataset) {
  recipe(Churn ~ ., data = dataset) %&gt;%
    step_string2factor(all_nominal(), -all_outcomes()) %&gt;%
    step_dummy(all_nominal(), -all_outcomes()) %&gt;%
    step_center(all_numeric()) %&gt;%
    step_scale(all_numeric()) %&gt;%
    prep(data = dataset)
}</code></pre>
</div>
<div id="estimate-the-model" class="section level3">
<h3>Estimate the model</h3>
<p>Switching to another model could not be simpler! All I need to do is to change the <strong>type of model</strong> to <code>random_forest</code> and add its hyper-parameters, change the <strong>set_engine</strong> argument to <code>ranger</code> and I’m ready to go.</p>
<p>I’m bundling all steps into a function that estimates the model across all folds, runs predictions and returns a convenient tibble with all the results. I need to add an extra step before the recipe “prepping” to maps the cross validation splits to the <code>analysis</code> and <code>assessment</code> functions. This will guide the iterations through the 10 folds.</p>
<pre class="r"><code>rf_fun &lt;- function(split, id, try, tree) {
   
  analysis_set &lt;- split %&gt;% analysis()
  analysis_prepped &lt;- analysis_set %&gt;% recipe_rf()
  analysis_baked &lt;- analysis_prepped %&gt;% bake(new_data = analysis_set)

  model_rf &lt;-
    rand_forest(
      mode = &quot;classification&quot;,
      mtry = try,
      trees = tree
    ) %&gt;%
    set_engine(&quot;ranger&quot;,
      importance = &quot;impurity&quot;
    ) %&gt;%
    fit(Churn ~ ., data = analysis_baked)

  assessment_set &lt;- split %&gt;% assessment()
  assessment_prepped &lt;- assessment_set %&gt;% recipe_rf()
  assessment_baked &lt;- assessment_prepped %&gt;% bake(new_data = assessment_set)

  tibble(
    &quot;id&quot; = id,
    &quot;truth&quot; = assessment_baked$Churn,
    &quot;prediction&quot; = model_rf %&gt;%
      predict(new_data = assessment_baked) %&gt;%
      unlist()
  )
  
}</code></pre>
</div>
<div id="performance-assessment-1" class="section level3">
<h3>Performance assessment</h3>
<p>All I have left to do is mapping the formula to a data frame.</p>
<pre class="r"><code>pred_rf &lt;- map2_df(
  .x = cross_val_tbl$splits,
  .y = cross_val_tbl$id,
  ~ rf_fun(split = .x, id = .y, try = 3, tree = 200)
)

head(pred_rf)
## # A tibble: 6 x 3
##   id     truth prediction
##   &lt;chr&gt;  &lt;fct&gt; &lt;fct&gt;     
## 1 Fold01 Yes   Yes       
## 2 Fold01 Yes   No        
## 3 Fold01 Yes   Yes       
## 4 Fold01 No    No        
## 5 Fold01 No    No        
## 6 Fold01 Yes   No</code></pre>
<p>I’ve found that <code>yardstick</code> has a very handy confusion matrix <code>summary</code> function, which returns an array of <strong>13 different metrics</strong> but in this case I want to see the four I used for the <code>glm</code> model.</p>
<pre class="r"><code>pred_rf %&gt;%
  conf_mat(truth, prediction) %&gt;%
  summary() %&gt;%
  select(-.estimator) %&gt;%
  filter(.metric %in%
    c(&quot;accuracy&quot;, &quot;precision&quot;, &quot;recall&quot;, &quot;f_meas&quot;)) %&gt;%
  kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="center">.metric</th>
<th align="center">.estimate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">accuracy</td>
<td align="center">0.7979026</td>
</tr>
<tr class="even">
<td align="center">precision</td>
<td align="center">0.8250436</td>
</tr>
<tr class="odd">
<td align="center">recall</td>
<td align="center">0.9186301</td>
</tr>
<tr class="even">
<td align="center">f_meas</td>
<td align="center">0.8693254</td>
</tr>
</tbody>
</table>
<p>The <code>random forest</code> model is performing in par with the simple <code>logistic regression</code>. Given the very basic feature engineering that I’ve carried out, there is scope to further improve the model but this is beyond the scope of this post.</p>
</div>
</div>
<div id="closing-considerations" class="section level2">
<h2>Closing considerations</h2>
<p>One of the great advantage of <code>tidymodels</code> is the flexibility and ease of access to every phase of the analysis workflow. Creating the modelling pipeline is a breeze and you can easily re-use the initial framework by changing model type with <code>parsnip</code> and data pre-processing with <code>recipes</code> and in no time you’re ready to check your new model’s performance with <code>yardstick</code>.</p>
<p>In any analysis you would typically audit several models and <code>parsnip</code> frees you up from having to learn the unique syntax of every modelling engine so that you can focus on finding the best solution for the problem at hand.</p>
<div id="code-repository" class="section level3">
<h3>Code repository</h3>
<p>The full R code can be found on <a href="https://github.com/DiegoUsaiUK/Classification_Churn_with_Parsnip">my GitHub profile</a></p>
</div>
<div id="references" class="section level3">
<h3>References</h3>
<ul>
<li>Big thanks to <em>Bruno Rodrigues</em> for the article that provided the inspiration for the big evaluation formula <a href="https://www.brodrigues.co/blog/2018-11-25-tidy_cv/">A tutorial on tidy cross-validation with R</a></li>
<li>More thanks to <em>Benjamin Sorensen</em> for his thoughtful piece on <a href="https://www.benjaminsorensen.me/post/modeling-with-parsnip-and-tidymodels/">Modeling with <code>parsnip</code> and <code>tidymodels</code></a></li>
<li>For an introduction to <a href="https://www.tidyverse.org/articles/2018/11/parsnip-0-0-1/"><code>parsnip</code></a></li>
</ul>
</div>
</div>
    
        </div>
    </section>

    <footer class="post-full-footer">
      <section class="author-card">
        <img class="author-profile-image" src="/img/Me_1.jpg" alt="Author" />
        <section class="author-card-content">
            <h4 class="author-card-name"><a href="/">Diego Usai</a></h4>
                <p></p>
        </section>
      </section>
    </footer>
</article>
    
    
    

  </div>
</main>


<aside class="read-next outer">
  <div class="inner">
    <div class="read-next-feed">      
      

      
      <article class="post-card post"> 
    
    <a class="post-card-image-link" href="/2019/09/loading-merging-and-joining-datasets/">
      <div class="post-card-image" style="background-image: url(/img/stefan-kunze-v8SjXTTI0GA-unsplash.jpg)"></div>
    </a>
    

    <div class="post-card-content">
      <a class="post-card-content-link" href="/2019/09/loading-merging-and-joining-datasets/">
          <header class="post-card-header">
              <span class="post-card-tags">
              #Data Cleaning 
              #Data Mining 
              #Data Wrangling 
              #Data Management  </span>
              
              <h2 class="post-card-title">Loading, Merging and Joining Datasets</h2>
          </header>
          <section class="post-card-excerpt">
              
                <p>This is the minimal coding necessary to assemble various data feeds and sort out the likes of variables naming &amp; new features creation plus some general housekeeping tasks. It includes general housekeeping tasks like sorting variables names, creating essential features and sorting out variables order
I will continue to add to this code should the need arise for other features to be created.
THE DATASET
library(tidyverse)library(lubridate)library(readr)The dataset I’m using here accompanies a Redbooks publication called Building 360-Degree Information Applications which is available as a free PDF download. ...  </p>
              
          </section>
      </a>

      <footer class="post-card-meta">
          <img class="author-profile-image" src="/img/Me_1.jpg" alt="Author" />
          <span class="post-card-author"><a href="/">Diego Usai</a></span>
      </footer>
    </div>
</article>
      
      
      <article class="post-card post"> 
    
    <a class="post-card-image-link" href="/2019/05/an-introduction-to-customer-segmentation/">
      <div class="post-card-image" style="background-image: url(/img/nick-karvounis-SWIoVDRZWUY-unsplash.jpg)"></div>
    </a>
    

    <div class="post-card-content">
      <a class="post-card-content-link" href="/2019/05/an-introduction-to-customer-segmentation/">
          <header class="post-card-header">
              <span class="post-card-tags">
              #Unsupervised Learning 
              #K Means Clustering 
              #Customer Segmentation 
              #Machine Learning 
              #Marketing Strategies  </span>
              
              <h2 class="post-card-title">An Introduction to Customer Segmentation: Using K-Means Clustering to Understand Marketing Response</h2>
          </header>
          <section class="post-card-excerpt">
              
                <p>Market segmentation refers to the process of dividing a consumer market of existing and/or potential customers into groups (or segments) based on shared attributes, interests, and behaviours.
For this mini-project I will use the popular K-Means clustering algorithm to segment customers based on their response to a series of marketing campaigns. The basic concept is that consumers who share common traits would respond to marketing communication in a similar way so that companies can reach out for each group in a relevant and effective way. ...  </p>
              
          </section>
      </a>

      <footer class="post-card-meta">
          <img class="author-profile-image" src="/img/Me_1.jpg" alt="Author" />
          <span class="post-card-author"><a href="/">Diego Usai</a></span>
      </footer>
    </div>
</article>
      
    </div>
  </div>
</aside>

<div class="floating-header">
  <div class="floating-header-logo">
    <a href="/">
      
      <span></span>
    </a>
  </div>
  <span class="floating-header-divider">&mdash;</span>
  <div class="floating-header-title">Modelling with Tidymodels and Parsnip - A Tidy Approach to a Classification Problem</div>
  <div class="floating-header-share">
    <div class="floating-header-share-label">Share this <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
     <path d="M7.5 15.5V4a1.5 1.5 0 1 1 3 0v4.5h2a1 1 0 0 1 1 1h2a1 1 0 0 1 1 1H18a1.5 1.5 0 0 1 1.5 1.5v3.099c0 .929-.13 1.854-.385 2.748L17.5 23.5h-9c-1.5-2-5.417-8.673-5.417-8.673a1.2 1.2 0 0 1 1.76-1.605L7.5 15.5zm6-6v2m-3-3.5v3.5m6-1v2"/></svg>
    </div>
    
    <a class="floating-header-share-tw" href="https://twitter.com/share?text=Modelling%20with%20Tidymodels%20and%20Parsnip%20-%20A%20Tidy%20Approach%20to%20a%20Classification%20Problem&amp;url=%2f2019%2f09%2fmodelling-with-tidymodels-and-parsnip%2f"
          onclick="window.open(this.href, 'share-twitter', 'width=550,height=235');return false;">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"/></svg>
      </a>
      <a class="floating-header-share-fb" href="https://www.facebook.com/sharer/sharer.php?u=%2f2019%2f09%2fmodelling-with-tidymodels-and-parsnip%2f"
          onclick="window.open(this.href, 'share-facebook','width=580,height=296');return false;">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M19 6h5V0h-5c-3.86 0-7 3.14-7 7v3H8v6h4v16h6V16h5l1-6h-6V7c0-.542.458-1 1-1z"/></svg>
      </a>
  </div>

  <progress class="progress" value="0">
    <div class="progress-container">
      <span class="progress-bar"></span>
    </div>
  </progress>
</div>



<footer class="site-footer outer">
  <div class="site-footer-content inner">
    <section class="copyright" style="line-height: 1.3em;">
      <a href="/">Diego Usai</a> © 2019 <br>
      <span style="font-size: 0.8em; color: #555;">Hugo port of <a href="https://github.com/TryGhost/Casper">Casper 2.1.7</a> by <a href="https://www.telematika.org">EM</a></span>
    </section>
    <nav class="site-footer-nav">
        <a href="/">Latest Posts</a>
        
        
        <a href="https://github.com/DiegoUsaiUK" target="_blank" rel="noopener">Github</a>
        <a href="https://www.linkedin.com/in/diegousaiuk" target="_blank" rel="noopener">LinkedIn</a>
        <a href="https://medium.com/@diegousaiuk" target="_blank" rel="noopener">Medium</a>
    </nav>  
  </div>
</footer>

</div>
<script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script type="text/javascript" src="//code.jquery.com/jquery-3.2.1.min.js"></script>
<script type="text/javascript" src="/js/jquery.fitvids.js"></script>

<script>hljs.initHighlightingOnLoad();</script>



    <script>





$(document).ready(function () {
    
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
    

    var progressBar = document.querySelector('progress');
    var header = document.querySelector('.floating-header');
    var title = document.querySelector('.post-full-title');

    var lastScrollY = window.scrollY;
    var lastWindowHeight = window.innerHeight;
    var lastDocumentHeight = $(document).height();
    var ticking = false;

    function onScroll() {
        lastScrollY = window.scrollY;
        requestTick();
    }

    function onResize() {
        lastWindowHeight = window.innerHeight;
        lastDocumentHeight = $(document).height();
        requestTick();
    }

    function requestTick() {
        if (!ticking) {
            requestAnimationFrame(update);
        }
        ticking = true;
    }

    function update() {
        var trigger = title.getBoundingClientRect().top + window.scrollY;
        var triggerOffset = title.offsetHeight + 35;
        var progressMax = lastDocumentHeight - lastWindowHeight;

        
        if (lastScrollY >= trigger + triggerOffset) {
            header.classList.add('floating-active');
        } else {
            header.classList.remove('floating-active');
        }

        progressBar.setAttribute('max', progressMax);
        progressBar.setAttribute('value', lastScrollY);

        ticking = false;
    }

    window.addEventListener('scroll', onScroll, {passive: true});
    window.addEventListener('resize', onResize, false);

    update();
});
</script>
</body></html>
